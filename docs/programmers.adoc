= JobRunr Control Extension - Programmer's Guide
:toc: left
:toclevels: 3
:sectnums:
:icons: font
:source-highlighter: rouge

== Introduction

This guide explains how to implement jobs that integrate with the JobRunr Control dashboard.
You will learn how to create configurable jobs, define parameters, implement batch jobs, and use lifecycle callbacks.

=== Prerequisites

* Java 21+
* Quarkus 3.30.8
* JobRunr Pro 8.4.1 with valid license
* JobRunr Control Extension dependency

=== Key Concepts

* **JobRequest**: A serializable record containing job parameters
* **JobRequestHandler**: A CDI bean that executes the job logic
* **@ConfigurableJob**: Annotation marking a job for dashboard discovery
* **@JobParameterDefinition**: Annotation defining parameter metadata

== Getting Started

=== Add Dependencies

[source,xml]
----
<dependency>
    <groupId>ch.css.jobrunr</groupId>
    <artifactId>jobrunr-control-extension</artifactId>
    <version>1.0.0-SNAPSHOT</version>
</dependency>
<dependency>
    <groupId>org.jobrunr</groupId>
    <artifactId>quarkus-jobrunr-pro</artifactId>
    <version>8.4.1</version>
</dependency>
----

=== Basic Job Structure

Every configurable job consists of two components:

. A **JobRequest** record containing parameters
. A **JobRequestHandler** CDI bean executing the job logic

[source,java]
----
// 1. JobRequest - defines parameters
public record MyJobRequest(
    String message,
    Integer count
) implements JobRequest {
    @Override
    public Class<MyJobHandler> getJobRequestHandler() {
        return MyJobHandler.class;
    }
}

// 2. JobRequestHandler - executes the job
@ApplicationScoped
public class MyJobHandler implements JobRequestHandler<MyJobRequest> {

    @ConfigurableJob
    @Override
    public void run(MyJobRequest request) {
        // Job implementation
        jobContext().logger().info("Processing: " + request.message());
    }
}
----

== Discovery Mechanism

[IMPORTANT]
====
Only jobs marked with `@ConfigurableJob` annotation are discovered by the dashboard.
Jobs without this annotation will not appear in the UI.
====

=== How Discovery Works

. At **Quarkus build time**, the extension scans for `JobRequestHandler` implementations
. It looks for the `@ConfigurableJob` annotation on the `run()` method
. It analyzes the `JobRequest` type parameter to extract field metadata
. Job definitions are stored in a registry for runtime access

=== What Gets Discovered

The scanner extracts:

* Job type (simple class name of handler)
* JobRequest class name
* Parameter definitions from record components
* Job settings from `@ConfigurableJob` annotation

== Implementing Simple Jobs

=== Step 1: Create the JobRequest

Define your parameters as a Java record implementing `JobRequest`:

[source,java]
----
package com.example.jobs;

import ch.css.jobrunr.control.annotations.JobParameterDefinition;
import org.jobrunr.jobs.lambdas.JobRequest;

public record ReportJobRequest(
    @JobParameterDefinition(defaultValue = "2024-01-01")
    LocalDate startDate,

    @JobParameterDefinition(defaultValue = "2024-12-31")
    LocalDate endDate,

    @JobParameterDefinition(defaultValue = "PDF")
    OutputFormat format
) implements JobRequest {

    @Override
    public Class<ReportJobHandler> getJobRequestHandler() {
        return ReportJobHandler.class;
    }
}
----

=== Step 2: Create the JobRequestHandler

Implement the handler as a CDI bean:

[source,java]
----
package com.example.jobs;

import ch.css.jobrunr.control.annotations.ConfigurableJob;
import jakarta.enterprise.context.ApplicationScoped;
import org.jobrunr.jobs.lambdas.JobRequestHandler;

@ApplicationScoped
public class ReportJobHandler implements JobRequestHandler<ReportJobRequest> {

    @ConfigurableJob(
        name = "Generate Report",
        labels = {"Reports", "Production"},
        retries = 3
    )
    @Override
    public void run(ReportJobRequest request) throws Exception {
        var logger = jobContext().logger();

        logger.info("Generating report from %s to %s",
            request.startDate(), request.endDate());

        // Your job logic here
        generateReport(request);

        logger.info("Report generated successfully");
    }

    private void generateReport(ReportJobRequest request) {
        // Implementation
    }
}
----

== Implementing Batch Jobs

Batch jobs process multiple items and track overall progress.

=== Step 1: Create the Main Batch Job

[source,java]
----
public record BatchImportJobRequest(
    @JobParameterDefinition(defaultValue = "1000")
    Integer batchSize,

    @JobParameterDefinition(defaultValue = "false")
    Boolean dryRun
) implements JobRequest {

    @Override
    public Class<BatchImportJobHandler> getJobRequestHandler() {
        return BatchImportJobHandler.class;
    }
}
----

[source,java]
----
@ApplicationScoped
public class BatchImportJobHandler implements JobRequestHandler<BatchImportJobRequest> {

    @Inject
    DataSource dataSource;

    @ConfigurableJob(
        isBatch = true,  // <-- Mark as batch job
        name = "Batch Import",
        labels = {"Import", "Batch"}
    )
    @Override
    public void run(BatchImportJobRequest request) throws Exception {
        var logger = jobContext().logger();

        // 1. Load items to process
        List<ImportItem> items = loadItems(request.batchSize());
        logger.info("Found %d items to process", items.size());

        // 2. Create child job requests
        List<ImportItemJobRequest> childRequests = items.stream()
            .map(item -> new ImportItemJobRequest(item.id(), request.dryRun()))
            .toList();

        // 3. Enqueue all child jobs
        BackgroundJobRequest.enqueue(childRequests.stream());

        logger.info("Enqueued %d child jobs", childRequests.size());
    }
}
----

=== Step 2: Create the Child Job

[source,java]
----
// Child job request (not annotated with @ConfigurableJob)
public record ImportItemJobRequest(
    Long itemId,
    Boolean dryRun
) implements JobRequest {

    @Override
    public Class<ImportItemJobHandler> getJobRequestHandler() {
        return ImportItemJobHandler.class;
    }
}
----

[source,java]
----
// Child job handler (no @ConfigurableJob - not shown in dashboard)
@ApplicationScoped
public class ImportItemJobHandler implements JobRequestHandler<ImportItemJobRequest> {

    @Override
    public void run(ImportItemJobRequest request) throws Exception {
        // Process individual item
        processItem(request.itemId(), request.dryRun());
    }
}
----

=== Batch Progress Tracking

The dashboard automatically tracks batch progress:

* **Total**: Number of child jobs created
* **Succeeded**: Completed child jobs
* **Failed**: Failed child jobs
* **Progress**: Percentage complete

Progress updates in real-time as child jobs complete.

== Parameter Configuration

=== @JobParameterDefinition Annotation

Use this annotation on record components to define parameter metadata:

[source,java]
----
public record MyJobRequest(
    @JobParameterDefinition(
        name = "Customer Name",    // Display name in UI
        defaultValue = "Default"   // Pre-filled value
    )
    String customerName,

    @JobParameterDefinition(defaultValue = "100")
    Integer maxResults
) implements JobRequest { ... }
----

.Annotation Properties
[cols="1,3"]
|===
|Property |Description

|`name`
|Display name in the UI (defaults to field name)

|`defaultValue`
|Default value as String (parsed to target type)
|===

=== Supported Parameter Types

[cols="1,2,2,2"]
|===
|Java Type |UI Input |Default Value Format |Notes

|`String`
|Text input (single line)
|`"any text"`
|Use for short text values

|`String` (with MULTILINE type)
|Multi-line textarea
|`"Line 1\nLine 2\nLine 3"`
|Use `@JobParameterDefinition(type = "MULTILINE")`

|`Integer` / `int`
|Number input
|`"42"`
|Whole numbers only

|`Long` / `long`
|Number input
|`"1234567890"`
|Large whole numbers

|`Boolean` / `boolean`
|Checkbox
|`"true"` or `"false"`
|True/false values

|`LocalDate`
|Date picker
|`"2024-01-15"` (ISO format)
|Date only, no time component

|`LocalDateTime`
|DateTime picker
|`"2024-01-15T14:30:00"` (ISO format)
|Date and time

|`Enum`
|Dropdown (single selection)
|`"ENUM_VALUE"` (constant name)
|Single enum value

|`EnumSet<E>`
|Multi-select dropdown
|`"VALUE1,VALUE2,VALUE3"`
|Multiple enum values (comma-separated)
|===

[IMPORTANT]
====
For multiline text parameters, explicitly specify the type in `@JobParameterDefinition`:
[source,java]
----
@JobParameterDefinition(
    name = "Description",
    type = "MULTILINE",  // <-- Required for textarea
    defaultValue = "Default text"
)
String description
----
====

[TIP]
====
EnumSet parameters allow users to select multiple enum values in the UI.
The values are stored as a comma-separated string and automatically converted to EnumSet at runtime.
====

=== Required vs Optional Parameters

By default, all parameters are **required**.
A parameter becomes optional when:

* It has a `defaultValue` specified in `@JobParameterDefinition`

[source,java]
----
public record MyJobRequest(
    String requiredParam,  // Required - no default

    @JobParameterDefinition(defaultValue = "optional")
    String optionalParam   // Optional - has default
) implements JobRequest { ... }
----

=== Enum Parameters

Enums are automatically rendered as dropdowns with all available values:

[source,java]
----
public enum Priority {
    LOW, MEDIUM, HIGH, CRITICAL
}

public record TaskJobRequest(
    @JobParameterDefinition(defaultValue = "MEDIUM")
    Priority priority
) implements JobRequest { ... }
----

=== Multi-Enum Parameters

Use `EnumSet<E>` to allow multiple enum selections:

[source,java]
----
public enum Feature {
    REPORTS, EXPORTS, NOTIFICATIONS, ANALYTICS
}

public record ConfigJobRequest(
    @JobParameterDefinition(defaultValue = "REPORTS,EXPORTS")
    EnumSet<Feature> enabledFeatures
) implements JobRequest { ... }
----

The UI renders a multi-select dropdown, and values are stored as comma-separated strings.

== Parameter Storage Strategies

The extension supports two parameter storage strategies to handle different job requirements.

=== Inline Storage (Default)

By default, all job parameters are stored **inline** directly in JobRunr's job metadata table.

.Characteristics
* Parameters serialized as part of the JobRequest
* Stored in JobRunr's `jobrunr_jobs` table
* No additional database tables required
* Suitable for most use cases

.When to Use
* Jobs with small to medium parameter sets (< 10 parameters)
* Parameters with reasonable sizes (strings < 1000 chars)
* Standard data types (String, Integer, Boolean, Date, Enum)

.Example: Inline Storage Job
[source,java]
----
public record SimpleReportRequest(
    @JobParameterDefinition(defaultValue = "2024-01-01")
    LocalDate startDate,

    @JobParameterDefinition(defaultValue = "PDF")
    OutputFormat format,

    @JobParameterDefinition(defaultValue = "admin@example.com")
    String recipient
) implements JobRequest {
    @Override
    public Class<SimpleReportHandler> getJobRequestHandler() {
        return SimpleReportHandler.class;
    }
}
----

=== External Storage

For jobs with **large or numerous parameters**, use the `@JobParameterSet` annotation to store parameters externally.

.Characteristics
* Parameters stored in separate database table (`jobrunr_control_parameter_sets`)
* Only parameter set ID stored in JobRequest
* Requires Hibernate ORM configuration
* Automatic cleanup of orphaned parameter sets

.When to Use
[cols="1,3"]
|===
|Scenario |Description

|**Large Text Fields**
|Multiline text parameters with substantial content (> 2000 characters)

|**Many Parameters**
|Jobs with 10+ parameters or complex nested structures

|**Parameter Reuse**
|Multiple job executions sharing the same parameter configuration

|**Database Limits**
|JobRunr's job metadata approaching database column size limits

|**Template Jobs**
|Jobs designed as reusable templates with varying parameter sets
|===

=== Implementing External Storage

==== Step 1: Enable Hibernate ORM

External storage requires Hibernate ORM to be configured:

[source,properties]
----
# application.properties
quarkus.hibernate-orm.enabled=true
quarkus.datasource.db-kind=postgresql
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/mydb
quarkus.datasource.username=myuser
quarkus.datasource.password=mypass
----

The extension automatically creates the `jobrunr_control_parameter_sets` table via Flyway migration.

==== Step 2: Use @JobParameterSet Annotation

Instead of defining parameters on record components, use `@JobParameterSet` annotation:

[source,java]
----
public record LargeDataJobRequest(
    @JobParameterSet({
        @JobParameterDefinition(
            name = "description",
            type = "MULTILINE",  // Multi-line text
            defaultValue = "Enter detailed description here..."
        ),
        @JobParameterDefinition(
            name = "documentContent",
            type = "MULTILINE",
            defaultValue = ""
        ),
        @JobParameterDefinition(
            name = "recipients",
            type = "MULTILINE",
            defaultValue = "user1@example.com\nuser2@example.com"
        ),
        @JobParameterDefinition(
            name = "processingMode",
            type = "com.example.ProcessingMode",
            defaultValue = "STANDARD"
        ),
        @JobParameterDefinition(
            name = "batchSize",
            type = "java.lang.Integer",
            defaultValue = "1000"
        )
    })
    String parameterSetId  // <-- Receives UUID reference to parameter set
) implements JobRequest {
    @Override
    public Class<LargeDataJobHandler> getJobRequestHandler() {
        return LargeDataJobHandler.class;
    }
}
----

[IMPORTANT]
====
.@JobParameterSet Requirements
* Must be applied to **exactly one** record component
* The annotated field **must** be of type `String`
* The `value` array must contain at least one `@JobParameterDefinition`
* Each definition must specify both `name` and `type` attributes
* The `type` must be the **fully qualified class name** (e.g., `"java.lang.String"`)
====

==== Step 3: Access Parameters in Handler

The handler receives the parameter set ID and can load parameters through the JobRunr context:

[source,java]
----
@ApplicationScoped
public class LargeDataJobHandler implements JobRequestHandler<LargeDataJobRequest> {

    @Inject
    ParameterSetLoaderPort parameterLoader;

    @ConfigurableJob(name = "Large Data Processor")
    @Override
    public void run(LargeDataJobRequest request) throws Exception {
        var logger = jobContext().logger();

        // Parameters are automatically loaded by the extension
        // and available in the job context metadata
        String description = getParameter("description", String.class);
        String documentContent = getParameter("documentContent", String.class);
        ProcessingMode mode = getParameter("processingMode", ProcessingMode.class);
        Integer batchSize = getParameter("batchSize", Integer.class);

        logger.info("Processing with mode: %s, batch size: %d", mode, batchSize);

        // Your job logic here
        processDocument(documentContent, mode, batchSize);
    }

    private <T> T getParameter(String name, Class<T> type) {
        // Helper method to extract parameters from job context
        // Implementation depends on how your job accesses metadata
        return jobContext().getMetadata(name, type);
    }
}
----

[TIP]
====
The extension provides a `ParameterSetLoaderPort` that you can inject to manually load parameter sets if needed:
[source,java]
----
@Inject
ParameterSetLoaderPort parameterLoader;

ParameterSet paramSet = parameterLoader.load(request.parameterSetId());
Map<String, Object> params = paramSet.parameters();
----
====

=== Parameter Type Specification

For external storage, the `type` attribute must be a fully qualified class name:

[cols="1,2"]
|===
|Java Type |Type Specification

|String
|`"java.lang.String"`

|Integer
|`"java.lang.Integer"`

|Boolean
|`"java.lang.Boolean"`

|LocalDate
|`"java.time.LocalDate"`

|LocalDateTime
|`"java.time.LocalDateTime"`

|Enum
|`"com.example.MyEnum"` (fully qualified)

|EnumSet
|`"java.util.EnumSet<com.example.MyEnum>"`

|Multiline (String)
|`"MULTILINE"` (special marker for textarea)
|===

=== Parameter Storage Cleanup

The extension provides automatic cleanup of orphaned parameter sets:

[source,properties]
----
# application.properties

# Enable automatic cleanup
jobrunr.control.parameter-storage.cleanup.enabled=true

# Retention period (days) before cleanup
jobrunr.control.parameter-storage.cleanup.retention-days=30
----

.Cleanup Behavior
* Runs periodically as a background job
* Removes parameter sets not referenced by any active job
* Parameter sets older than retention period are deleted
* Cleanup respects the `last_accessed_at` timestamp

=== Choosing the Right Strategy

[cols="1,2,2"]
|===
|Aspect |Inline Storage |External Storage

|**Complexity**
|Simple, no setup required
|Requires Hibernate ORM setup

|**Performance**
|Faster for small parameters
|Better for large parameters

|**Database Impact**
|Single table (jobrunr_jobs)
|Two tables (jobs + parameter_sets)

|**Size Limits**
|Limited by JobRunr metadata size
|No practical limits

|**Use Cases**
|Standard jobs with few parameters
|Jobs with many or large parameters

|**Maintenance**
|No cleanup needed
|Automatic cleanup available
|===

=== Best Practices

==== Inline Storage
* Keep parameter count low (< 10)
* Limit string lengths (< 1000 chars)
* Use for jobs with fixed parameter schemas
* Prefer for simple configuration jobs

==== External Storage
* Use for jobs with 10+ parameters
* Ideal for template jobs with varying configs
* Use MULTILINE type for large text fields
* Enable cleanup to prevent storage bloat
* Consider parameter set reuse for similar jobs

==== Type Annotations
* Always specify `type` for external storage parameters
* Use fully qualified class names for custom types
* Mark multiline text explicitly with `type = "MULTILINE"`
* Document expected parameter formats in job comments

== Lifecycle Callbacks

=== Success Callback

Implement `JobRequestOnSuccessFactory` to chain a job after successful completion:

[source,java]
----
public record MyBatchJobRequest(
    Integer batchSize
) implements JobRequest, JobRequestOnSuccessFactory {

    @Override
    public Class<MyBatchJobHandler> getJobRequestHandler() {
        return MyBatchJobHandler.class;
    }

    @Override
    public JobRequest createOnSuccessJobRequest(
            JobRequestId jobRequestId,
            JobRequest jobRequest) {
        return new NotifySuccessRequest(
            jobRequestId.asUUID(),
            "Batch completed successfully"
        );
    }
}
----

The success callback job is automatically scheduled when the main job completes successfully.

=== Failure Callback

Implement `JobRequestOnFailureFeactory` to chain a job when execution fails:

[source,java]
----
public record MyBatchJobRequest(
    Integer batchSize
) implements JobRequest, JobRequestOnFailureFeactory {

    @Override
    public Class<MyBatchJobHandler> getJobRequestHandler() {
        return MyBatchJobHandler.class;
    }

    @Override
    public JobRequest createOnFailureJobRequest(
            JobRequestId jobRequestId,
            JobRequest jobRequest) {
        return new AlertFailureRequest(
            jobRequestId.asUUID(),
            "Batch job failed!"
        );
    }
}
----

=== Combining Callbacks

You can implement both interfaces:

[source,java]
----
public record MyBatchJobRequest(Integer batchSize)
    implements JobRequest,
               JobRequestOnSuccessFactory,
               JobRequestOnFailureFeactory {

    @Override
    public JobRequest createOnSuccessJobRequest(...) {
        return new SuccessNotificationRequest(...);
    }

    @Override
    public JobRequest createOnFailureJobRequest(...) {
        return new FailureAlertRequest(...);
    }
}
----

== @ConfigurableJob Options

=== Full Annotation Reference

[source,java]
----
@ConfigurableJob(
    // Job identification
    name = "My Job Name",

    // Batch job flag
    isBatch = false,

    // Retry configuration
    retries = 3,

    // Labels for filtering/organization
    labels = {"Production", "Daily", "Reports"},

    // Queue assignment
    queue = "high-priority",

    // Concurrency control
    mutex = "my-unique-mutex",
    rateLimiter = "my-rate-limiter",

    // Server routing (JobRunr Pro)
    runOnServerWithTag = "dedicated-server",

    // Timeout configuration (ISO 8601 duration)
    processTimeOut = "PT30M",  // 30 minutes

    // Cleanup configuration
    deleteOnSuccess = "PT5M!PT10H",  // Move to deleted after 5min, purge after 10h
    deleteOnFailure = "PT24H!PT48H"  // Move to deleted after 24h, purge after 48h
)
----

.Annotation Properties
[cols="1,1,3"]
|===
|Property |Type |Description

|`name`
|String
|Display name for the job

|`isBatch`
|boolean
|Mark as batch job (default: false)

|`retries`
|int
|Number of retry attempts (default: use global setting)

|`labels`
|String[]
|Labels for filtering and organization

|`queue`
|String
|Queue name for job routing

|`mutex`
|String
|Mutex for exclusive execution

|`rateLimiter`
|String
|Rate limiter ID

|`runOnServerWithTag`
|String
|Server tag for routing

|`processTimeOut`
|String
|Maximum processing time (ISO 8601 duration)

|`deleteOnSuccess`
|String
|Cleanup timing for succeeded jobs

|`deleteOnFailure`
|String
|Cleanup timing for failed jobs
|===

== Template Jobs

Template jobs are reusable job configurations managed separately from regular scheduled jobs.
They serve as blueprints that can be cloned and executed multiple times with varying parameters.

=== Understanding Templates

From a developer's perspective, templates are:

* **Regular configurable jobs** - They use the same `@ConfigurableJob` annotation
* **Never executed directly** - Marked with "template" label in the dashboard
* **Cloneable blueprints** - Each execution creates a new job instance
* **Managed separately** - Have dedicated UI and use cases

=== When to Design for Templates

Design a job as a template when:

* It will be executed repeatedly with varying parameters
* External systems need to trigger it on-demand
* Each execution should be tracked independently
* The job represents a reusable workflow pattern

=== Template-Ready Job Design

There's no special code required for templates.
Any configurable job can be used as a template:

[source,java]
----
public record CustomerReportRequest(
    @JobParameterDefinition(defaultValue = "STANDARD")
    ReportType reportType,

    @JobParameterDefinition(defaultValue = "2024-01-01")
    LocalDate startDate,

    @JobParameterDefinition(defaultValue = "2024-12-31")
    LocalDate endDate
) implements JobRequest {
    @Override
    public Class<CustomerReportHandler> getJobRequestHandler() {
        return CustomerReportHandler.class;
    }
}

@ApplicationScoped
public class CustomerReportHandler implements JobRequestHandler<CustomerReportRequest> {

    @ConfigurableJob(
        name = "Customer Report Generator",
        labels = {"Reports", "Customer"}
    )
    @Override
    public void run(CustomerReportRequest request) {
        var logger = jobContext().logger();
        logger.info("Generating %s report from %s to %s",
            request.reportType(),
            request.startDate(),
            request.endDate());

        // Implementation
        generateReport(request);
    }
}
----

=== Creating Templates Programmatically

While templates are typically created through the UI, you can create them programmatically using the template use cases:

[source,java]
----
@Inject
CreateTemplateUseCase createTemplateUseCase;

// Create a template
UUID templateId = createTemplateUseCase.execute(
    "CustomerReportHandler",  // Job type
    "Daily-Customer-Report",  // Template name
    Map.of(                   // Default parameters
        "reportType", "STANDARD",
        "startDate", "2024-01-01",
        "endDate", "2024-12-31"
    )
);
----

=== Executing Templates Programmatically

Use the `ExecuteTemplateUseCase` to clone and execute a template:

[source,java]
----
@Inject
ExecuteTemplateUseCase executeTemplateUseCase;

// Execute template with overrides
UUID newJobId = executeTemplateUseCase.execute(
    templateId,              // Template to clone
    "20240127-special",     // Postfix for job name
    Map.of(                 // Parameter overrides
        "reportType", "DETAILED",
        "startDate", "2024-01-27"
    )
);

// Result: New job named "Daily-Customer-Report-20240127-special" is created and executed
----

=== Template Best Practices for Developers

==== Use Meaningful Default Values

Templates rely heavily on defaults.
Make them useful:

[source,java]
----
public record DataImportRequest(
    // Good: Specific, common value
    @JobParameterDefinition(defaultValue = "1000")
    Integer batchSize,

    // Good: Today's date is often the right choice
    @JobParameterDefinition(defaultValue = "#{T(java.time.LocalDate).now()}")
    LocalDate processDate,

    // Good: Safe default for production
    @JobParameterDefinition(defaultValue = "false")
    Boolean dryRun
) implements JobRequest { ... }
----

==== Document Expected Overrides

Add clear documentation about which parameters should be overridden:

[source,java]
----
/**
 * Template for customer data import.
 * <p>
 * Typical override pattern:
 * - customerId: REQUIRED - specify customer to import
 * - fullSync: Optional - set true for complete refresh
 * - batchSize: Usually keep default (1000)
 */
public record CustomerImportRequest(
    @JobParameterDefinition
    String customerId,

    @JobParameterDefinition(defaultValue = "false")
    Boolean fullSync,

    @JobParameterDefinition(defaultValue = "1000")
    Integer batchSize
) implements JobRequest { ... }
----

==== Consider Template Naming Patterns

Design job names that work well with postfix:

[source,java]
----
// Good template names (work well with date postfix):
// "Daily-Report" → "Daily-Report-20240127"
// "Customer-Import" → "Customer-Import-customer-123"
// "Weekly-Cleanup" → "Weekly-Cleanup-2024-W04"

@ConfigurableJob(
    name = "Daily Sales Report",  // Base name
    labels = {"Reports", "Daily"}
)
----

==== Design for Idempotency

Templates are often re-executed.
Make jobs idempotent:

[source,java]
----
@Override
public void run(CustomerImportRequest request) {
    var logger = jobContext().logger();

    // Check if already processed
    if (importRepository.exists(request.customerId(), request.processDate())) {
        logger.warn("Import already processed for customer %s on %s",
            request.customerId(), request.processDate());
        return;
    }

    // Process import
    processImport(request);

    // Mark as processed
    importRepository.markProcessed(request.customerId(), request.processDate());
}
----

=== Template Architecture

Templates are managed through dedicated use cases in the `ch.css.jobrunr.control.application.template` package:

[cols="1,3"]
|===
|Use Case |Purpose

|`GetTemplatesUseCase`
|Retrieve all template jobs

|`GetTemplateByIdUseCase`
|Retrieve a specific template

|`CreateTemplateUseCase`
|Create a new template

|`UpdateTemplateUseCase`
|Update an existing template

|`DeleteTemplateUseCase`
|Delete a template

|`ExecuteTemplateUseCase`
|Clone and execute a template
|===

=== Template vs Regular Job

[cols="1,2,2"]
|===
|Aspect |Template Job |Regular Scheduled Job

|**Creation**
|Created through Templates UI
|Created through Scheduled Jobs UI

|**Execution**
|Cloned then executed
|Executed directly

|**Reusability**
|High - each execution is independent
|Low - single job instance

|**Labels**
|Always has "template" label
|Custom labels

|**Use Case**
|Recurring pattern with variations
|Specific one-time or scheduled task
|===

== REST API Usage

=== Starting a Scheduled Job

[source,bash]
----
POST /q/jobrunr-control/api/jobs/{jobId}/start
Content-Type: application/json

{
  "param1": "override-value",
  "param2": 42
}
----

Response:

[source,json]
----
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Job started successfully"
}
----

=== Executing a Template

[source,bash]
----
POST /q/jobrunr-control/api/templates/{templateId}/start
Content-Type: application/json

{
  "postfix": "20240127-customer-123",
  "parameters": {
    "customerId": "123",
    "reportType": "DETAILED"
  }
}
----

Response:

[source,json]
----
{
  "jobId": "660e8400-e29b-41d4-a716-446655440001",
  "message": "Template job started successfully"
}
----

[NOTE]
====
The returned `jobId` is the ID of the newly created job, not the template.
The template remains unchanged and can be executed again.
====

=== Cloning and Starting a Job (Deprecated)

[NOTE]
====
This endpoint is deprecated.
Use templates instead for better organization and management.
====

[source,bash]
----
POST /q/jobrunr-control/api/jobs
Content-Type: application/json

{
  "cloneFromId": "550e8400-e29b-41d4-a716-446655440000",
  "suffix": "20240126",
  "parameters": {
    "batchSize": 500
  }
}
----

=== Getting Job Status

[source,bash]
----
GET /q/jobrunr-control/api/jobs/{jobId}
----

Response:

[source,json]
----
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "jobName": "Daily Import",
  "jobType": "BatchImportJobHandler",
  "status": "PROCESSING",
  "startedAt": "2024-01-26T10:30:00Z",
  "finishedAt": null,
  "batchProgress": {
    "total": 1000,
    "succeeded": 450,
    "failed": 5,
    "pending": 545,
    "progress": 45.5
  }
}
----

== Best Practices

=== Job Design

. **Keep jobs idempotent**: Jobs may be retried, so ensure repeated execution is safe
. **Use meaningful names**: Help operators understand job purpose
. **Log progress**: Use `jobContext().logger()` for dashboard-visible logs
. **Handle interruption**: Check `Thread.currentThread().isInterrupted()` for long operations

=== Parameter Design

. **Use sensible defaults**: Reduce configuration effort for common cases
. **Validate early**: Check parameters at the start of execution
. **Document format**: Use clear naming for date/time formats

=== Batch Job Design

. **Balance batch size**: Too small = overhead, too large = long-running
. **Handle partial failures**: Design for some child jobs failing
. **Use progress logging**: Log batch milestones for visibility

=== Error Handling

[source,java]
----
@Override
public void run(MyJobRequest request) throws Exception {
    var logger = jobContext().logger();

    try {
        // Main logic
        processData(request);
    } catch (RecoverableException e) {
        // Let JobRunr retry
        logger.warn("Recoverable error, will retry: " + e.getMessage());
        throw e;
    } catch (PermanentException e) {
        // Don't retry - fail immediately
        logger.error("Permanent failure: " + e.getMessage());
        throw new JobRunrException("Job failed permanently", e);
    }
}
----

== Troubleshooting

=== Job Not Appearing in Dashboard

* Verify `@ConfigurableJob` annotation is on the `run()` method
* Check that the handler implements `JobRequestHandler<T>`
* Ensure the JobRequest implements `JobRequest` interface
* Rebuild the application (discovery happens at build time)

=== Parameters Not Showing

* Verify JobRequest is a Java record
* Check that record components have correct types
* Rebuild after adding `@JobParameterDefinition`

=== Serialization Errors

* Ensure all parameter types are Jackson-serializable
* Check that enums have a default constructor
* Verify complex types are properly annotated

=== Build-Time Errors

* Check for circular dependencies in JobRequest
* Verify all referenced classes are on the classpath
* Review Quarkus build output for scanner warnings
