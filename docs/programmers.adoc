= JobRunr Control Extension - Programmer's Guide
:toc: left
:toclevels: 3
:sectnums:
:icons: font
:source-highlighter: rouge

== Introduction

This guide explains how to implement jobs that integrate with the JobRunr Control dashboard.
You will learn how to create configurable jobs, define parameters, implement batch jobs, and use lifecycle callbacks.

=== Prerequisites

* Java 21+
* Quarkus 3.31.2
* JobRunr Pro 8.4.2 with valid license
* JobRunr Control Extension dependency

=== Key Concepts

* **JobRequest**: A serializable record containing job parameters
* **JobRequestHandler**: A CDI bean that executes the job logic
* **@ConfigurableJob**: Annotation marking a job for dashboard discovery
* **@JobParameterDefinition**: Annotation defining parameter metadata

== Getting Started

=== Add Dependencies

[source,xml]
----
<dependency>
    <groupId>ch.css.quarkus</groupId>
    <artifactId>quarkus-jobrunr-control</artifactId>
    <version>1.3.1</version>
</dependency>
----

=== Basic Job Structure

Every configurable job consists of two components:

. A **JobRequest** record containing parameters
. A **JobRequestHandler** CDI bean executing the job logic

[source,java]
----
// 1. JobRequest - defines parameters
public record MyJobRequest(
    String message,
    Integer count
) implements JobRequest {
    @Override
    public Class<MyJobHandler> getJobRequestHandler() {
        return MyJobHandler.class;
    }
}

// 2. JobRequestHandler - executes the job
@ApplicationScoped
public class MyJobHandler implements JobRequestHandler<MyJobRequest> {

    @ConfigurableJob
    @Override
    public void run(MyJobRequest request) {
        // Job implementation
        jobContext().logger().info("Processing: " + request.message());
    }
}
----

== Security Configuration

The JobRunr Control extension provides role-based access control (RBAC) for both the Web UI and REST API.

=== Role Overview

The extension uses **two distinct role sets**:

==== Web UI Roles

The Web UI at `/q/jobrunr-control` uses these roles:

[cols="1,3"]
|===
|Role |Permissions

|`viewer`
|Read-only access to scheduled jobs and execution history

|`configurator`
|All viewer permissions plus: create, edit, delete scheduled jobs

|`admin`
|All configurator permissions plus: execute jobs immediately
|===

==== REST API Roles

The REST API at `/q/jobrunr-control/api` uses separate roles:

[cols="1,3"]
|===
|Role |Permissions

|`api-reader`
|Read job status via GET endpoints (e.g., `GET /jobs/{jobId}`)

|`api-executor`
|All api-reader permissions plus: start jobs via POST endpoints (e.g., `POST /jobs/{jobId}/start`)

|`admin`
|Full access to all API operations (shared with UI)
|===

[NOTE]
====
**Why separate role sets?**

This separation allows organizations to:

* Grant operators UI access without exposing API credentials
* Issue API keys to external systems (e.g., CI/CD pipelines) with narrower permissions
* Apply different authentication mechanisms (form login for UI, bearer tokens for API)

The `admin` role is shared across both surfaces for administrative operations.
====

=== Configuring Security

==== Development Mode

In development mode, grant all roles for easy testing:

[source,properties]
----
# application.properties
%dev.quarkus.security.users.embedded.enabled=true
%dev.quarkus.security.users.embedded.plain-text=true
%dev.quarkus.security.users.embedded.users.admin=admin
%dev.quarkus.security.users.embedded.roles.admin=admin,viewer,configurator,api-reader,api-executor
----

==== Production Mode with OIDC

For production, use OIDC/OAuth2 with role mapping:

[source,properties]
----
# application.properties
quarkus.oidc.auth-server-url=https://your-keycloak.com/realms/your-realm
quarkus.oidc.client-id=jobrunr-control
quarkus.oidc.credentials.secret=${OIDC_CLIENT_SECRET}

# Map OIDC roles to application roles
quarkus.oidc.roles.source=accesstoken
quarkus.oidc.roles.role-claim-path=realm_access/roles
----

Configure your OIDC provider (Keycloak, Auth0, etc.) to issue the following roles:

* For UI users: `viewer`, `configurator`, `admin`
* For API clients: `api-reader`, `api-executor`, `admin`

==== Production Mode with Basic Auth

For simple deployments, use basic authentication:

[source,properties]
----
# application.properties
quarkus.security.users.embedded.enabled=true
quarkus.security.users.embedded.plain-text=false

# UI user with configurator role
quarkus.security.users.embedded.users.operator=${OPERATOR_PASSWORD}
quarkus.security.users.embedded.roles.operator=viewer,configurator

# API client with executor role
quarkus.security.users.embedded.users.api-client=${API_CLIENT_PASSWORD}
quarkus.security.users.embedded.roles.api-client=api-reader,api-executor

# Admin user with full access
quarkus.security.users.embedded.users.admin=${ADMIN_PASSWORD}
quarkus.security.users.embedded.roles.admin=admin,viewer,configurator,api-reader,api-executor
----

[WARNING]
====
In production, always use hashed passwords.
Generate BCrypt hashes using:

[source,bash]
----
./mvnw quarkus:generate-config -Dquarkus.security.users.embedded.password=your-password
----
====

=== API Authentication Examples

==== Using Basic Auth

[source,bash]
----
curl -X POST \
  -H "Authorization: Basic $(echo -n 'api-client:password' | base64)" \
  -H "Content-Type: application/json" \
  -d '{"parameters": {"customerId": "123"}}' \
  https://your-server/q/jobrunr-control/api/jobs/{jobId}/start
----

==== Using Bearer Token (OIDC)

[source,bash]
----
# 1. Obtain token from your OIDC provider
TOKEN=$(curl -X POST https://your-keycloak.com/realms/your-realm/protocol/openid-connect/token \
  -d "client_id=jobrunr-control" \
  -d "client_secret=${CLIENT_SECRET}" \
  -d "grant_type=client_credentials" \
  | jq -r '.access_token')

# 2. Use token to call API
curl -X POST \
  -H "Authorization: Bearer ${TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{"parameters": {"customerId": "123"}}' \
  https://your-server/q/jobrunr-control/api/jobs/{jobId}/start
----

=== Testing Security Configuration

Verify your role configuration:

[source,bash]
----
# Test UI access (should work with viewer role)
curl -u viewer:password https://your-server/q/jobrunr-control

# Test API read access (should work with api-reader role)
curl -u api-client:password https://your-server/q/jobrunr-control/api/jobs/{jobId}

# Test API execute access (should work with api-executor role)
curl -X POST -u api-client:password \
  https://your-server/q/jobrunr-control/api/jobs/{jobId}/start

# Test admin access (should work for all endpoints)
curl -u admin:password https://your-server/q/jobrunr-control/api/jobs/{jobId}
----

== Discovery Mechanism

[IMPORTANT]
====
Only jobs marked with `@ConfigurableJob` annotation are discovered by the dashboard.
Jobs without this annotation will not appear in the UI.
====

=== How Discovery Works

. At **Quarkus build time**, the extension scans for `JobRequestHandler` implementations
. It looks for the `@ConfigurableJob` annotation on the `run()` method
. It analyzes the `JobRequest` type parameter to extract field metadata
. Job definitions are stored in a registry for runtime access

=== What Gets Discovered

The scanner extracts:

* Job type (simple class name of handler)
* JobRequest class name
* Parameter definitions from record components
* Job settings from `@ConfigurableJob` annotation

== Implementing Simple Jobs

=== Step 1: Create the JobRequest

Define your parameters as a Java record implementing `JobRequest`:

[source,java]
----
package com.example.jobs;

import ch.css.jobrunr.control.annotations.JobParameterDefinition;
import org.jobrunr.jobs.lambdas.JobRequest;

public record ReportJobRequest(
    @JobParameterDefinition(defaultValue = "2024-01-01")
    LocalDate startDate,

    @JobParameterDefinition(defaultValue = "2024-12-31")
    LocalDate endDate,

    @JobParameterDefinition(defaultValue = "PDF")
    OutputFormat format
) implements JobRequest {

    @Override
    public Class<ReportJobHandler> getJobRequestHandler() {
        return ReportJobHandler.class;
    }
}
----

=== Step 2: Create the JobRequestHandler

Implement the handler as a CDI bean:

[source,java]
----
package com.example.jobs;

import ch.css.jobrunr.control.annotations.ConfigurableJob;
import jakarta.enterprise.context.ApplicationScoped;
import org.jobrunr.jobs.lambdas.JobRequestHandler;

@ApplicationScoped
public class ReportJobHandler implements JobRequestHandler<ReportJobRequest> {

    @ConfigurableJob(
        name = "Generate Report",
        labels = {"Reports", "Production"},
        retries = 3
    )
    @Override
    public void run(ReportJobRequest request) throws Exception {
        var logger = jobContext().logger();

        logger.info("Generating report from %s to %s",
            request.startDate(), request.endDate());

        // Your job logic here
        generateReport(request);

        logger.info("Report generated successfully");
    }

    private void generateReport(ReportJobRequest request) {
        // Implementation
    }
}
----

== Implementing Batch Jobs

Batch jobs process multiple items and track overall progress.

=== Step 1: Create the Main Batch Job

[source,java]
----
public record BatchImportJobRequest(
    @JobParameterDefinition(defaultValue = "1000")
    Integer batchSize,

    @JobParameterDefinition(defaultValue = "false")
    Boolean dryRun
) implements JobRequest {

    @Override
    public Class<BatchImportJobHandler> getJobRequestHandler() {
        return BatchImportJobHandler.class;
    }
}
----

[source,java]
----
@ApplicationScoped
public class BatchImportJobHandler implements JobRequestHandler<BatchImportJobRequest> {

    @Inject
    DataSource dataSource;

    @ConfigurableJob(
        isBatch = true,  // <-- Mark as batch job
        name = "Batch Import",
        labels = {"Import", "Batch"}
    )
    @Override
    public void run(BatchImportJobRequest request) throws Exception {
        var logger = jobContext().logger();

        // 1. Load items to process
        List<ImportItem> items = loadItems(request.batchSize());
        logger.info("Found %d items to process", items.size());

        // 2. Create child job requests
        List<ImportItemJobRequest> childRequests = items.stream()
            .map(item -> new ImportItemJobRequest(item.id(), request.dryRun()))
            .toList();

        // 3. Enqueue all child jobs
        BackgroundJobRequest.enqueue(childRequests.stream());

        logger.info("Enqueued %d child jobs", childRequests.size());
    }
}
----

=== Step 2: Create the Child Job

Child jobs need the parent batch job ID to establish the parent-child relationship.
Pass it as a parameter in the child job request:

[source,java]
----
// Child job request - receives parentBatchJobId from parent
public record ImportItemJobRequest(
    Long itemId,
    Boolean dryRun,
    UUID parentBatchJobId  // <-- Required for batch relationship
) implements JobRequest {

    @Override
    public Class<ImportItemJobHandler> getJobRequestHandler() {
        return ImportItemJobHandler.class;
    }
}
----

The parent batch job obtains its own ID and passes it to each child:

[source,java]
----
// In the parent batch job handler:
var parentBatchJobId = ThreadLocalJobContext.getJobContext().getJobId();

List<ImportItemJobRequest> childRequests = items.stream()
    .map(item -> new ImportItemJobRequest(item.id(), request.dryRun(), parentBatchJobId))
    .toList();

BackgroundJobRequest.enqueue(childRequests.stream());
----

[source,java]
----
// Child job handler (no @ConfigurableJob - not shown in dashboard)
@ApplicationScoped
public class ImportItemJobHandler implements JobRequestHandler<ImportItemJobRequest> {

    @Override
    public void run(ImportItemJobRequest request) throws Exception {
        // Process individual item
        processItem(request.itemId(), request.dryRun());
    }
}
----

=== Loading External Parameters in Batch Jobs

Batch jobs using `@JobParameterSet` load their parameters from external storage using the job's own UUID:

[source,java]
----
@ConfigurableJob(isBatch = true, labels = {"ExternalData"})
@Override
public void run(ExternalDataBatchJobRequest request) {
    // Load parameters from external storage using the job's own UUID
    UUID parameterSetId = ThreadLocalJobContext.getJobContext().getJobId();
    ParameterSet parameterSet = parameterStorageService.findById(parameterSetId)
            .orElseThrow(() -> new IllegalStateException(
                    "Parameter set not found: " + parameterSetId));

    Integer numberOfChildJobs = parameterSet.getInteger("numberOfChildJobs");
    String stringParam = parameterSet.getString("stringExternalParameter");
    // ... use parameters to create and enqueue child jobs
}
----

=== Success and Failure Callbacks

Use `JobRequestOnSuccessFactory` and `JobRequestOnFailureFactory` on the batch job's request to chain callback jobs after batch completion.
The callback receives the original `JobRequest`, which provides access to the parent batch job's context:

[source,java]
----
public record BatchImportJobRequest(
    @JobParameterDefinition(defaultValue = "1000") Integer batchSize,
    @JobParameterDefinition(defaultValue = "false") Boolean dryRun
) implements JobRequest, JobRequestOnSuccessFactory, JobRequestOnFailureFactory {

    @Override
    public JobRequest createOnSuccessJobRequest(JobRequestId jobRequestId, JobRequest jobRequest) {
        return new BatchSuccessRequest(jobRequest);
    }

    @Override
    public JobRequest createOnFailureJobRequest(JobRequestId jobRequestId, JobRequest jobRequest) {
        return new BatchFailureRequest(jobRequest);
    }
}
----

In the callback handler, use `ThreadLocalJobContext.getJobContext().getAwaitedJob()` to retrieve the parent batch job ID:

[source,java]
----
@ApplicationScoped
public class BatchSuccessHandler implements JobRequestHandler<BatchSuccessRequest> {

    @Job(name = "Batch Success Post-Processing", retries = 0)
    @Override
    public void run(BatchSuccessRequest request) {
        // Access the parent batch job that triggered this callback
        UUID parentJobId = ThreadLocalJobContext.getJobContext().getAwaitedJob();
        LOG.infof("Batch job %s completed successfully", parentJobId);
    }
}
----

=== Batch Progress Tracking

The dashboard automatically tracks batch progress:

* **Total**: Number of child jobs created
* **Succeeded**: Completed child jobs
* **Failed**: Failed child jobs
* **Progress**: Percentage complete

Progress updates in real-time as child jobs complete.

== Parameter Configuration

=== @JobParameterDefinition Annotation

Use this annotation on record components to define parameter metadata:

[source,java]
----
public record MyJobRequest(
    @JobParameterDefinition(
        name = "Customer Name",    // Display name in UI
        defaultValue = "Default"   // Pre-filled value
    )
    String customerName,

    @JobParameterDefinition(defaultValue = "100")
    Integer maxResults
) implements JobRequest { ... }
----

.Annotation Properties
[cols="1,3"]
|===
|Property |Description

|`name`
|Display name in the UI (defaults to field name)

|`defaultValue`
|Default value as String (parsed to target type)
|===

=== Supported Parameter Types

[cols="1,2,2,2"]
|===
|Java Type |UI Input |Default Value Format |Notes

|`String`
|Text input (single line)
|`"any text"`
|Use for short text values

|`String` (with MULTILINE type)
|Multi-line textarea
|`"Line 1\nLine 2\nLine 3"`
|Use `@JobParameterDefinition(type = "MULTILINE")`

|`Integer` / `int`
|Number input
|`"42"`
|Whole numbers only

|`Long` / `long`
|Number input
|`"1234567890"`
|Large whole numbers

|`Double` / `double`
|Decimal number input
|`"3.14159"`
|Floating-point numbers

|`Float` / `float`
|Decimal number input
|`"2.71828"`
|Floating-point numbers (mapped to DOUBLE)

|`Boolean` / `boolean`
|Checkbox
|`"true"` or `"false"`
|True/false values

|`LocalDate`
|Date picker
|`"2024-01-15"` (ISO format)
|Date only, no time component

|`LocalDateTime`
|DateTime picker
|`"2024-01-15T14:30:00"` (ISO format)
|Date and time

|`Enum`
|Dropdown (single selection)
|`"ENUM_VALUE"` (constant name)
|Single enum value

|`EnumSet<E>`
|Multi-select dropdown
|`"VALUE1,VALUE2,VALUE3"`
|Multiple enum values (comma-separated)
|===

[IMPORTANT]
====
For multiline text parameters, explicitly specify the type in `@JobParameterDefinition`:

[source,java]
----
@JobParameterDefinition(
    name = "Description",
    type = "MULTILINE",  // <-- Required for textarea
    defaultValue = "Default text"
)
String description
----
====

[TIP]
====
EnumSet parameters allow users to select multiple enum values in the UI.
The values are stored as a comma-separated string and automatically converted to EnumSet at runtime.
====

=== Required vs Optional Parameters

By default, all parameters are **required**.
A parameter becomes optional when:

* It has a `defaultValue` specified in `@JobParameterDefinition`

[source,java]
----
public record MyJobRequest(
    String requiredParam,  // Required - no default

    @JobParameterDefinition(defaultValue = "optional")
    String optionalParam   // Optional - has default
) implements JobRequest { ... }
----

=== Enum Parameters

Enums are automatically rendered as dropdowns with all available values:

[source,java]
----
public enum Priority {
    LOW, MEDIUM, HIGH, CRITICAL
}

public record TaskJobRequest(
    @JobParameterDefinition(defaultValue = "MEDIUM")
    Priority priority
) implements JobRequest { ... }
----

=== Multi-Enum Parameters

Use `EnumSet<E>` to allow multiple enum selections:

[source,java]
----
public enum Feature {
    REPORTS, EXPORTS, NOTIFICATIONS, ANALYTICS
}

public record ConfigJobRequest(
    @JobParameterDefinition(defaultValue = "REPORTS,EXPORTS")
    EnumSet<Feature> enabledFeatures
) implements JobRequest { ... }
----

The UI renders a multi-select dropdown, and values are stored as comma-separated strings.

== Parameter Storage Strategies

The extension supports two parameter storage strategies to handle different job requirements.

=== Inline Storage (Default)

By default, all job parameters are stored **inline** directly in JobRunr's job metadata table.

.Characteristics
* Parameters serialized as part of the JobRequest
* Stored in JobRunr's `jobrunr_jobs` table
* No additional database tables required
* Suitable for most use cases

.When to Use
* Jobs with small to medium parameter sets (< 10 parameters)
* Parameters with reasonable sizes (strings < 1000 chars)
* Standard data types (String, Integer, Boolean, Date, Enum)

.Example: Inline Storage Job
[source,java]
----
public record SimpleReportRequest(
    @JobParameterDefinition(defaultValue = "2024-01-01")
    LocalDate startDate,

    @JobParameterDefinition(defaultValue = "PDF")
    OutputFormat format,

    @JobParameterDefinition(defaultValue = "admin@example.com")
    String recipient
) implements JobRequest {
    @Override
    public Class<SimpleReportHandler> getJobRequestHandler() {
        return SimpleReportHandler.class;
    }
}
----

=== External Storage

For jobs with **large or numerous parameters**, use the `@JobParameterSet` annotation to store parameters externally.

.Characteristics
* Parameters stored in separate database table (`jobrunr_control_parameter_sets`)
* Parameter set ID equals the job's own UUID (1:1 relationship)
* Job handler retrieves parameters via `ThreadLocalJobContext.getJobContext().getJobId()`
* Requires Hibernate ORM configuration
* Automatic cleanup of orphaned parameter sets

.When to Use
[cols="1,3"]
|===
|Scenario |Description

|**Large Text Fields**
|Multiline text parameters with substantial content (> 2000 characters)

|**Many Parameters**
|Jobs with 10+ parameters or complex nested structures

|**Parameter Reuse**
|Multiple job executions sharing the same parameter configuration

|**Database Limits**
|JobRunr's job metadata approaching database column size limits

|**Template Jobs**
|Jobs designed as reusable templates with varying parameter sets
|===

=== Implementing External Storage

==== Step 1: Enable Hibernate ORM

External storage requires Hibernate ORM to be configured:

[source,properties]
----
# application.properties
quarkus.hibernate-orm.enabled=true
quarkus.datasource.db-kind=postgresql
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/mydb
quarkus.datasource.username=myuser
quarkus.datasource.password=mypass
----

For **development with H2**, use standard H2 configuration:

[source,properties]
----
# application.properties (H2 development setup)
quarkus.datasource.db-kind=h2
quarkus.datasource.jdbc.url=jdbc:h2:mem:devdb;DB_CLOSE_DELAY=-1
----

[IMPORTANT]
====
**H2 Compatibility Mode Not Supported:** H2 must run in standard mode (not Oracle, MySQL, or PostgreSQL modes) because JobRunr generates database-specific SQL.
If you need Oracle compatibility testing, use an actual Oracle database.
====

You must create the `jobrunr_control_parameter_sets` table manually.
See SQL scripts in `docs/sql/` directory for your database platform (PostgreSQL, Oracle, MySQL, or H2).

==== Step 2: Use @JobParameterSet Annotation

Instead of defining parameters on record components, use `@JobParameterSet` annotation on the record type:

[source,java]
----
@JobParameterSet({
    @JobParameterDefinition(
        name = "description",
        type = "MULTILINE",  // Multi-line text
        defaultValue = "Enter detailed description here..."
    ),
    @JobParameterDefinition(
        name = "documentContent",
        type = "MULTILINE",
        defaultValue = ""
    ),
    @JobParameterDefinition(
        name = "recipients",
        type = "MULTILINE",
        defaultValue = "user1@example.com\nuser2@example.com"
    ),
    @JobParameterDefinition(
        name = "processingMode",
        type = "com.example.ProcessingMode",
        defaultValue = "STANDARD"
    ),
    @JobParameterDefinition(
        name = "batchSize",
        type = "java.lang.Integer",
        defaultValue = "1000"
    )
})
public record LargeDataJobRequest() implements JobRequest {
    @Override
    public Class<LargeDataJobHandler> getJobRequestHandler() {
        return LargeDataJobHandler.class;
    }
}
----

[IMPORTANT]
====
.@JobParameterSet Requirements
* Must be applied to the **JobRequest record type** (not a record component)
* The record should have **no component fields** for external parameters
* The `value` array must contain at least one `@JobParameterDefinition`
* Each definition must specify both `name` and `type` attributes
* The `type` must be the **fully qualified class name** (e.g., `"java.lang.String"`)
====

==== Step 3: Access Parameters in Handler

The handler retrieves external parameters using the job's own UUID via `ThreadLocalJobContext`:

[source,java]
----
@ApplicationScoped
public class LargeDataJobHandler implements JobRequestHandler<LargeDataJobRequest> {

    @Inject
    ParameterStorageService parameterStorageService;

    @ConfigurableJob(name = "Large Data Processor")
    @Override
    public void run(LargeDataJobRequest request) throws Exception {
        var logger = jobContext().logger();

        // Load parameters from external storage using the job's own UUID
        UUID parameterSetId = ThreadLocalJobContext.getJobContext().getJobId();
        ParameterSet parameterSet = parameterStorageService.findById(parameterSetId)
                .orElseThrow(() -> new IllegalStateException(
                        "Parameter set not found: " + parameterSetId));

        Map<String, Object> params = parameterSet.parameters();
        String description = (String) params.get("description");
        String documentContent = (String) params.get("documentContent");
        Integer batchSize = (Integer) params.get("batchSize");

        logger.info("Processing with batch size: %d", batchSize);

        // Your job logic here
        processDocument(documentContent, batchSize);
    }
}
----

[TIP]
====
The parameter set ID is always equal to the job's own UUID.
The extension stores parameters under this ID when the job is scheduled, making the lookup straightforward.
====

=== Parameter Type Specification

For external storage, the `type` attribute must be a fully qualified class name:

[cols="1,2"]
|===
|Java Type |Type Specification

|String
|`"java.lang.String"`

|Integer
|`"java.lang.Integer"`

|Boolean
|`"java.lang.Boolean"`

|LocalDate
|`"java.time.LocalDate"`

|LocalDateTime
|`"java.time.LocalDateTime"`

|Enum
|`"com.example.MyEnum"` (fully qualified)

|EnumSet
|`"java.util.EnumSet<com.example.MyEnum>"`

|Multiline (String)
|`"MULTILINE"` (special marker for textarea)
|===

=== Parameter Storage Cleanup

The extension provides automatic cleanup of orphaned parameter sets:

[source,properties]
----
# application.properties

# Enable automatic cleanup
quarkus.jobrunr-control.parameter-storage.cleanup.enabled=true

# Retention period (days) before cleanup
quarkus.jobrunr-control.parameter-storage.cleanup.retention-days=30
----

.Cleanup Behavior
* Runs periodically as a background job
* Removes parameter sets not referenced by any active job
* Parameter sets older than retention period are deleted
* Cleanup respects the `last_accessed_at` timestamp

=== Choosing the Right Strategy

[cols="1,2,2"]
|===
|Aspect |Inline Storage |External Storage

|**Complexity**
|Simple, no setup required
|Requires Hibernate ORM setup

|**Performance**
|Faster for small parameters
|Better for large parameters

|**Database Impact**
|Single table (jobrunr_jobs)
|Two tables (jobs + parameter_sets)

|**Size Limits**
|Limited by JobRunr metadata size
|No practical limits

|**Use Cases**
|Standard jobs with few parameters (< 10), strings < 1000 chars
|Jobs with 10+ parameters, large text fields, or template jobs

|**Maintenance**
|No cleanup needed
|Automatic cleanup available
|===

.Tips
* For external storage, always specify `type` as the fully qualified class name
* Mark multiline text explicitly with `type = "MULTILINE"`
* Enable cleanup (`quarkus.jobrunr-control.parameter-storage.cleanup.enabled=true`) to prevent storage bloat

== Lifecycle Callbacks

The extension provides two interfaces for chaining follow-up jobs after completion or failure.
For a practical example with batch jobs, see <<Success and Failure Callbacks>>.

=== Success Callback

Implement `JobRequestOnSuccessFactory` on your `JobRequest` to chain a job after successful completion.
The callback is automatically scheduled when the main job completes successfully.

=== Failure Callback

Implement `JobRequestOnFailureFeactory` on your `JobRequest` to chain a job when execution fails.

=== Combining Callbacks

A single `JobRequest` can implement both interfaces to handle both outcomes.

=== Accessing the Parent Job

In callback handlers, use `ThreadLocalJobContext.getJobContext().getAwaitedJob()` to retrieve the UUID of the parent job that triggered the callback.

=== Job Chain Status Evaluation

When using lifecycle callbacks (`continueWith()` or `onFailure()`), the JobRunr Control extension automatically evaluates the overall status of the entire job chain.

==== How It Works

The extension analyzes all continuation jobs recursively to determine:

* **Chain Completion**: Whether all relevant jobs in the chain have finished executing
* **Overall Status**: The aggregated status representing the entire workflow

==== Status Evaluation Rules

[cols="1,3"]
|===
|Condition |Chain Status

|All relevant leaf jobs succeeded
|`SUCCEEDED`

|Any executed leaf job failed
|`FAILED`

|Any leaf job is still running (ENQUEUED, PROCESSING, PROCESSED)
|`PROCESSING` (in-progress)

|Parent job has no continuation jobs
|Status equals parent job status
|===

==== Relevant Leaf Jobs

The evaluator determines which jobs are "relevant" based on the parent job's status:

* **continueWith()** jobs: Only execute if parent **SUCCEEDED** → included in evaluation when parent succeeds
* **onFailure()** jobs: Only execute if parent **FAILED** → included in evaluation when parent fails

[source,java]
----
public record DataProcessingRequest(String dataPath)
    implements JobRequest, JobRequestOnSuccessFactory, JobRequestOnFailureFeactory {

    @Override
    public JobRequest createOnSuccessJobRequest(...) {
        // This job only executes if parent succeeds
        return new NotifySuccessRequest(...);
    }

    @Override
    public JobRequest createOnFailureJobRequest(...) {
        // This job only executes if parent fails
        return new AlertFailureRequest(...);
    }
}
----

==== Benefits

* **Accurate Workflow Visibility**: See the true status of complex job chains in the UI
* **Automatic Tracking**: No manual status management required
* **Deep Chain Support**: Works with nested continuation chains of any depth
* **Conditional Execution**: Properly handles success-only and failure-only continuations

[TIP]
====
The job chain status is visible in the JobRunr Control dashboard execution history.
For detailed per-job analysis, click the deep-link to JobRunr Pro Dashboard.
====

== @ConfigurableJob Options

=== Full Annotation Reference

[source,java]
----
@ConfigurableJob(
    // Job identification
    name = "My Job Name",

    // Batch job flag
    isBatch = false,

    // Retry configuration
    retries = 3,

    // Labels for filtering/organization
    labels = {"Production", "Daily", "Reports"},

    // Queue assignment
    queue = "high-priority",

    // Concurrency control
    mutex = "my-unique-mutex",
    rateLimiter = "my-rate-limiter",

    // Server routing (JobRunr Pro)
    runOnServerWithTag = "dedicated-server",

    // Timeout configuration (ISO 8601 duration)
    processTimeOut = "PT30M",  // 30 minutes

    // Cleanup configuration
    deleteOnSuccess = "PT5M!PT10H",  // Move to deleted after 5min, purge after 10h
    deleteOnFailure = "PT24H!PT48H"  // Move to deleted after 24h, purge after 48h
)
----

.Annotation Properties
[cols="1,1,3"]
|===
|Property |Type |Description

|`name`
|String
|Display name for the job

|`isBatch`
|boolean
|Mark as batch job (default: false)

|`retries`
|int
|Number of retry attempts (default: use global setting)

|`labels`
|String[]
|Labels for filtering and organization

|`queue`
|String
|Queue name for job routing

|`mutex`
|String
|Mutex for exclusive execution

|`rateLimiter`
|String
|Rate limiter ID

|`runOnServerWithTag`
|String
|Server tag for routing

|`processTimeOut`
|String
|Maximum processing time (ISO 8601 duration)

|`deleteOnSuccess`
|String
|Cleanup timing for succeeded jobs

|`deleteOnFailure`
|String
|Cleanup timing for failed jobs
|===

== Template Jobs

Template jobs are reusable job configurations managed separately from regular scheduled jobs.
They serve as blueprints that can be cloned and executed multiple times with varying parameters.

=== Understanding Templates

From a developer's perspective, templates are:

* **Regular configurable jobs** - They use the same `@ConfigurableJob` annotation
* **Never executed directly** - Marked with "template" label in the dashboard
* **Cloneable blueprints** - Each execution creates a new job instance
* **Managed separately** - Have dedicated UI and use cases

=== When to Design for Templates

Design a job as a template when:

* It will be executed repeatedly with varying parameters
* External systems need to trigger it on-demand
* Each execution should be tracked independently
* The job represents a reusable workflow pattern

=== Template-Ready Job Design

There's no special code required for templates.
Any configurable job can be used as a template:

[source,java]
----
public record CustomerReportRequest(
    @JobParameterDefinition(defaultValue = "STANDARD")
    ReportType reportType,

    @JobParameterDefinition(defaultValue = "2024-01-01")
    LocalDate startDate,

    @JobParameterDefinition(defaultValue = "2024-12-31")
    LocalDate endDate
) implements JobRequest {
    @Override
    public Class<CustomerReportHandler> getJobRequestHandler() {
        return CustomerReportHandler.class;
    }
}

@ApplicationScoped
public class CustomerReportHandler implements JobRequestHandler<CustomerReportRequest> {

    @ConfigurableJob(
        name = "Customer Report Generator",
        labels = {"Reports", "Customer"}
    )
    @Override
    public void run(CustomerReportRequest request) {
        var logger = jobContext().logger();
        logger.info("Generating %s report from %s to %s",
            request.reportType(),
            request.startDate(),
            request.endDate());

        // Implementation
        generateReport(request);
    }
}
----

=== Creating Templates Programmatically

While templates are typically created through the UI, you can create them programmatically using the template use cases:

[source,java]
----
@Inject
CreateTemplateUseCase createTemplateUseCase;

// Create a template
UUID templateId = createTemplateUseCase.execute(
    "CustomerReportHandler",  // Job type
    "Daily-Customer-Report",  // Template name
    Map.of(                   // Default parameters
        "reportType", "STANDARD",
        "startDate", "2024-01-01",
        "endDate", "2024-12-31"
    )
);
----

=== Executing Templates Programmatically

Use the `ExecuteTemplateUseCase` to clone and execute a template:

[source,java]
----
@Inject
ExecuteTemplateUseCase executeTemplateUseCase;

// Execute template with overrides
UUID newJobId = executeTemplateUseCase.execute(
    templateId,              // Template to clone
    "20240127-special",     // Postfix for job name
    Map.of(                 // Parameter overrides
        "reportType", "DETAILED",
        "startDate", "2024-01-27"
    )
);

// Result: New job named "Daily-Customer-Report-20240127-special" is created and executed
----

=== Template Best Practices for Developers

==== Use Meaningful Default Values

Templates rely heavily on defaults.
Make them useful:

[source,java]
----
public record DataImportRequest(
    // Good: Specific, common value
    @JobParameterDefinition(defaultValue = "1000")
    Integer batchSize,

    // Good: Today's date is often the right choice
    @JobParameterDefinition(defaultValue = "#{T(java.time.LocalDate).now()}")
    LocalDate processDate,

    // Good: Safe default for production
    @JobParameterDefinition(defaultValue = "false")
    Boolean dryRun
) implements JobRequest { ... }
----

==== Document Expected Overrides

Add clear documentation about which parameters should be overridden:

[source,java]
----
/**
 * Template for customer data import.
 * <p>
 * Typical override pattern:
 * - customerId: REQUIRED - specify customer to import
 * - fullSync: Optional - set true for complete refresh
 * - batchSize: Usually keep default (1000)
 */
public record CustomerImportRequest(
    @JobParameterDefinition
    String customerId,

    @JobParameterDefinition(defaultValue = "false")
    Boolean fullSync,

    @JobParameterDefinition(defaultValue = "1000")
    Integer batchSize
) implements JobRequest { ... }
----

==== Consider Template Naming Patterns

Design job names that work well with postfix:

[source,java]
----
// Good template names (work well with date postfix):
// "Daily-Report" → "Daily-Report-20240127"
// "Customer-Import" → "Customer-Import-customer-123"
// "Weekly-Cleanup" → "Weekly-Cleanup-2024-W04"

@ConfigurableJob(
    name = "Daily Sales Report",  // Base name
    labels = {"Reports", "Daily"}
)
----

==== Design for Idempotency

Templates are often re-executed.
Make jobs idempotent:

[source,java]
----
@Override
public void run(CustomerImportRequest request) {
    var logger = jobContext().logger();

    // Check if already processed
    if (importRepository.exists(request.customerId(), request.processDate())) {
        logger.warn("Import already processed for customer %s on %s",
            request.customerId(), request.processDate());
        return;
    }

    // Process import
    processImport(request);

    // Mark as processed
    importRepository.markProcessed(request.customerId(), request.processDate());
}
----

=== Template Architecture

Templates are managed through dedicated use cases in the `ch.css.jobrunr.control.application.template` package:

[cols="1,3"]
|===
|Use Case |Purpose

|`GetTemplatesUseCase`
|Retrieve all template jobs

|`GetTemplateByIdUseCase`
|Retrieve a specific template

|`CreateTemplateUseCase`
|Create a new template

|`UpdateTemplateUseCase`
|Update an existing template

|`DeleteTemplateUseCase`
|Delete a template

|`ExecuteTemplateUseCase`
|Clone and execute a template
|===

=== Template vs Regular Job

[cols="1,2,2"]
|===
|Aspect |Template Job |Regular Scheduled Job

|**Creation**
|Created through Templates UI
|Created through Scheduled Jobs UI

|**Execution**
|Cloned then executed
|Executed directly

|**Reusability**
|High - each execution is independent
|Low - single job instance

|**Labels**
|Always has "template" label
|Custom labels

|**Use Case**
|Recurring pattern with variations
|Specific one-time or scheduled task
|===

== REST API Usage

=== Starting a Scheduled Job

[source,bash]
----
POST /q/jobrunr-control/api/jobs/{jobId}/start
Content-Type: application/json

{
  "param1": "override-value",
  "param2": 42
}
----

Response:

[source,json]
----
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "message": "Job started successfully"
}
----

=== Executing a Template

Templates are started via the same `/jobs/{jobId}/start` endpoint.
The API detects that the job is a template, clones it, and executes the clone.

[source,bash]
----
POST /q/jobrunr-control/api/jobs/{templateId}/start
Content-Type: application/json

{
  "postfix": "20240127-customer-123",
  "parameters": {
    "customerId": "123",
    "reportType": "DETAILED"
  }
}
----

Response:

[source,json]
----
{
  "jobId": "660e8400-e29b-41d4-a716-446655440001",
  "message": "Template job started successfully"
}
----

[NOTE]
====
The returned `jobId` is the ID of the newly created job, not the template.
The template remains unchanged and can be executed again.
====

=== Getting Job Status

[source,bash]
----
GET /q/jobrunr-control/api/jobs/{jobId}
----

Response:

[source,json]
----
{
  "jobId": "550e8400-e29b-41d4-a716-446655440000",
  "jobName": "Daily Import",
  "jobType": "BatchImportJobHandler",
  "status": "PROCESSING",
  "startedAt": "2024-01-26T10:30:00Z",
  "finishedAt": null,
  "batchProgress": {
    "total": 1000,
    "succeeded": 450,
    "failed": 5,
    "pending": 545,
    "progress": 45.5
  }
}
----

== Best Practices

=== Job Design

. **Keep jobs idempotent**: Jobs may be retried, so ensure repeated execution is safe
. **Use meaningful names**: Help operators understand job purpose
. **Log progress**: Use `jobContext().logger()` for dashboard-visible logs
. **Handle interruption**: Check `Thread.currentThread().isInterrupted()` for long operations

=== Parameter Design

. **Use sensible defaults**: Reduce configuration effort for common cases
. **Validate early**: Check parameters at the start of execution
. **Document format**: Use clear naming for date/time formats

=== Batch Job Design

. **Balance batch size**: Too small = overhead, too large = long-running
. **Handle partial failures**: Design for some child jobs failing
. **Use progress logging**: Log batch milestones for visibility

=== Error Handling

[source,java]
----
@Override
public void run(MyJobRequest request) throws Exception {
    var logger = jobContext().logger();

    try {
        // Main logic
        processData(request);
    } catch (RecoverableException e) {
        // Let JobRunr retry
        logger.warn("Recoverable error, will retry: " + e.getMessage());
        throw e;
    } catch (PermanentException e) {
        // Don't retry - fail immediately
        logger.error("Permanent failure: " + e.getMessage());
        throw new JobRunrException("Job failed permanently", e);
    }
}
----

== Troubleshooting

=== Job Not Appearing in Dashboard

* Verify `@ConfigurableJob` annotation is on the `run()` method
* Check that the handler implements `JobRequestHandler<T>`
* Ensure the JobRequest implements `JobRequest` interface
* Rebuild the application (discovery happens at build time)

=== Parameters Not Showing

* Verify JobRequest is a Java record
* Check that record components have correct types
* Rebuild after adding `@JobParameterDefinition`

=== Serialization Errors

* Ensure all parameter types are Jackson-serializable
* Check that enums have a default constructor
* Verify complex types are properly annotated

=== Build-Time Errors

* Check for circular dependencies in JobRequest
* Verify all referenced classes are on the classpath
* Review Quarkus build output for scanner warnings





